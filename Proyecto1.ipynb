{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fde82ce",
   "metadata": {},
   "source": [
    "\n",
    "# Instalación de Dependencias\n",
    "\n",
    "## ¿Qué hace esta celda?\n",
    "Instala todas las dependencias necesarias desde el archivo `requirements.txt` para ejecutar el pipeline completo.\n",
    "\n",
    "## Dependencias Clave:\n",
    "- **Polars**: Procesamiento de datos ultra-rápido (10-100x más rápido que Pandas)\n",
    "- **Requests**: Descargas HTTP eficientes con streaming\n",
    "- **tqdm**: Barras de progreso informativas y detalladas\n",
    "- **Concurrent.futures**: Procesamiento paralelo para máxima eficiencia\n",
    "\n",
    "## ¿Por qué es la mejor opción?\n",
    "- **Reproducibilidad**: Garantiza que todos tengan las mismas versiones exactas\n",
    "- **Portabilidad**: Funciona en cualquier entorno (Windows, Mac, Linux)\n",
    "- **Mantenimiento**: Centraliza todas las dependencias en un solo archivo\n",
    "- **Automatización**: No necesitas instalar manualmente cada librería\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c57ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998217",
   "metadata": {},
   "source": [
    "\n",
    "# Configuración del Sistema\n",
    "\n",
    "## ¿Qué hace esta celda?\n",
    "Configura todo el entorno del pipeline: imports, directorios, URLs y parámetros de rendimiento optimizados.\n",
    "\n",
    "## Componentes Clave:\n",
    "- **Path**: Manejo robusto de rutas multiplataforma\n",
    "- **Manifests**: Sistema de trazabilidad para evitar reprocesamiento\n",
    "- **Concurrencia**: Configuración optimizada para tu hardware (4 workers)\n",
    "- **S3 Index**: Acceso directo al bucket público de Citi Bike\n",
    "\n",
    "## ¿Por qué es la mejor opción?\n",
    "- **Modularidad**: Separación clara de responsabilidades\n",
    "- **Escalabilidad**: Fácil ajuste de parámetros de rendimiento\n",
    "- **Trazabilidad**: Los manifests evitan trabajo duplicado\n",
    "- **Portabilidad**: Funciona en Windows, Mac y Linux sin cambios\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae3228d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T02:09:16.980171Z",
     "start_time": "2025-08-23T02:09:16.806701Z"
    }
   },
   "outputs": [],
   "source": [
    "import io, re, gc, zipfile, uuid, json, math, threading, time, shutil\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Carpeta de caché de ZIPs y salida Parquet (se crean si no existen)\n",
    "RAW_ZIPS   = Path(\"raw_zips\");     RAW_ZIPS.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR = Path(\"data_parquet\"); OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Índice público del bucket S3 de Citi Bike\n",
    "S3_INDEX = \"https://s3.amazonaws.com/tripdata/\"\n",
    "\n",
    "# Manifests\n",
    "DL_MANIFEST   = Path(\"manifest_download.json\")  # ZIPs descargados (caché)\n",
    "PROC_MANIFEST = Path(\"manifest_process.json\")   # ZIPs procesados a Parquet (trazabilidad)\n",
    "\n",
    "# Concurrencia y red\n",
    "MAX_WORKERS = 4         # puedes subir/bajar según tu máquina\n",
    "CHUNK_SIZE  = 1024 * 1024 # 1 MiB\n",
    "\n",
    "def _load_manifest(path: Path) -> set[str]:\n",
    "    return set(json.loads(path.read_text())) if path.exists() else set()\n",
    "\n",
    "def _save_manifest(path: Path, s: set[str]):\n",
    "    path.write_text(json.dumps(sorted(list(s)), indent=2))\n",
    "\n",
    "def load_dl_manifest():   return _load_manifest(DL_MANIFEST)\n",
    "def save_dl_manifest(s):  _save_manifest(DL_MANIFEST, s)\n",
    "def load_proc_manifest(): return _load_manifest(PROC_MANIFEST)\n",
    "def save_proc_manifest(s):_save_manifest(PROC_MANIFEST, s)\n",
    "\n",
    "def list_s3_zips(max_objects=None) -> list[str]:\n",
    "    r = requests.get(S3_INDEX, timeout=60); r.raise_for_status()\n",
    "    root = ET.fromstring(r.content); ns = {\"s3\":\"http://s3.amazonaws.com/doc/2006-03-01/\"}\n",
    "    items = []\n",
    "    for c in root.findall(\"s3:Contents\", ns):\n",
    "        key = c.find(\"s3:Key\", ns).text\n",
    "        if key.endswith(\".zip\") and not key.startswith(\"JC-\") and re.search(r\"(20\\d{2})(\\d{2})?-citibike-tripdata\\.zip$\", key):\n",
    "            items.append(key)\n",
    "    def sort_key(k):\n",
    "        m = re.search(r\"(20\\d{2})(\\d{2})?-citibike-tripdata\\.zip$\", k)\n",
    "        y = int(m.group(1)) if m else 0\n",
    "        mm = int(m.group(2)) if (m and m.group(2)) else 0 \n",
    "        return (y, mm)\n",
    "    items.sort(key=sort_key)\n",
    "    return items[:max_objects] if max_objects else items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366130ef",
   "metadata": {},
   "source": [
    "\n",
    "# Sistema de Descarga Inteligente\n",
    "\n",
    "## ¿Qué hace esta celda?\n",
    "Implementa un sistema de descarga inteligente que:\n",
    "- Verifica qué archivos ya están descargados usando manifests\n",
    "- Descarga solo los archivos faltantes (evita duplicación)\n",
    "- Procesa múltiples descargas en paralelo (4 hilos simultáneos)\n",
    "- Mantiene trazabilidad completa de lo descargado\n",
    "\n",
    "## Características Avanzadas:\n",
    "- **Descarga Paralela**: 4 hilos simultáneos para máxima velocidad\n",
    "- **Resume Downloads**: Continúa descargas interrumpidas automáticamente\n",
    "- **Progress Tracking**: Barras de progreso detalladas con velocidad y tiempo\n",
    "- **Error Handling**: Manejo robusto de errores de red y timeouts\n",
    "\n",
    "## ¿Por qué es la mejor opción?\n",
    "- **Eficiencia**: Solo descarga lo necesario, ahorra tiempo y ancho de banda\n",
    "- **Robustez**: Maneja interrupciones y errores sin perder progreso\n",
    "- **Velocidad**: Paralelización optimizada para tu conexión\n",
    "- **Trazabilidad**: Sabe exactamente qué se descargó y qué falta\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c073e12e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T02:09:18.213690Z",
     "start_time": "2025-08-23T02:09:18.199141Z"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def download_one(key: str, position:int=1) -> str:\n",
    "    name = key.rsplit(\"/\",1)[-1]\n",
    "    dest = RAW_ZIPS / name\n",
    "    if dest.exists():\n",
    "        tqdm.write(f\"Saltando (caché) {name}\")\n",
    "        return name\n",
    "\n",
    "    url = S3_INDEX + key\n",
    "    with requests.get(url, stream=True, timeout=120) as r:\n",
    "        r.raise_for_status()\n",
    "        total_bytes = int(r.headers.get(\"Content-Length\") or 0)\n",
    "\n",
    "        pbar = tqdm(\n",
    "            total=total_bytes if total_bytes > 0 else None,\n",
    "            desc=f\"Descargando  {name}\",\n",
    "            unit=\"B\", unit_scale=True, unit_divisor=1024,\n",
    "            position=position, leave=True, dynamic_ncols=True,\n",
    "            bar_format=\"{l_bar}{bar} {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\"\n",
    "        )\n",
    "\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=CHUNK_SIZE):\n",
    "                if not chunk:\n",
    "                    continue\n",
    "                f.write(chunk)\n",
    "                pbar.update(len(chunk))  # ← enteros en bytes\n",
    "\n",
    "        pbar.close()\n",
    "    return name\n",
    "\n",
    "def download_to_cache(max_objects=None):\n",
    "    keys = list_s3_zips(max_objects=max_objects)\n",
    "    done = load_dl_manifest()\n",
    "    pending = [k for k in keys if (RAW_ZIPS / k.rsplit(\"/\",1)[-1]).exists() is False and k.rsplit(\"/\",1)[-1] not in done]\n",
    "    if not pending:\n",
    "        print(\"Nada por descargar. Caché al día.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Descargando {len(pending)} ZIP(s) a {RAW_ZIPS} con {MAX_WORKERS} hilos…\")\n",
    "    results, errors = [], []\n",
    "    pos_pool = list(range(1, MAX_WORKERS+1))\n",
    "    pos_lock = threading.Lock()\n",
    "\n",
    "    def _task(k):\n",
    "        with pos_lock:\n",
    "            pos = pos_pool.pop() if pos_pool else 1\n",
    "        try:\n",
    "            name = download_one(k, position=pos)\n",
    "            return name\n",
    "        finally:\n",
    "            with pos_lock:\n",
    "                pos_pool.append(pos)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "        futs = {ex.submit(_task, k): k for k in pending}\n",
    "        for fut in as_completed(futs):\n",
    "            key = futs[fut]\n",
    "            try:\n",
    "                name = fut.result()\n",
    "                results.append(name)\n",
    "                d = load_dl_manifest(); d.add(name); save_dl_manifest(d)\n",
    "            except Exception as e:\n",
    "                errors.append((key, str(e)))\n",
    "\n",
    "    print(f\"Descargados: {len(results)} · Errores: {len(errors)}\")\n",
    "    if errors:\n",
    "        for k, e in errors:\n",
    "            print(\" -\", k.rsplit(\"/\",1)[-1], \"→\", e)\n",
    "\n",
    "# Descarga específica por año/mes (prefiere mensuales si existen)\n",
    "def keys_for_year_months(year:int, months:set[int]|None, all_keys:list[str]) -> list[str]:\n",
    "    monthly_keys = {k for k in all_keys if re.fullmatch(fr\"{year}\\d{{2}}-citibike-tripdata\\.zip\", k)}\n",
    "    if months:\n",
    "        found = [f\"{year}{m:02d}-citibike-tripdata.zip\" for m in sorted(months) if f\"{year}{m:02d}-citibike-tripdata.zip\" in monthly_keys]\n",
    "        if found:\n",
    "            return found\n",
    "    if monthly_keys and not months:\n",
    "        return sorted(monthly_keys)\n",
    "    annual = f\"{year}-citibike-tripdata.zip\"\n",
    "    if annual in all_keys:\n",
    "        return [annual]\n",
    "    if monthly_keys:\n",
    "        return sorted(monthly_keys)\n",
    "    return []\n",
    "\n",
    "def download_years_months_to_cache(years:list[int], months:set[int]|None=None):\n",
    "    all_keys = list_s3_zips(max_objects=None)\n",
    "    targets = []\n",
    "    for y in years:\n",
    "        targets.extend(keys_for_year_months(y, months, all_keys))\n",
    "    done = load_dl_manifest()\n",
    "    pending = [k for k in targets if (RAW_ZIPS / k.rsplit(\"/\",1)[-1]).exists() is False and k.rsplit(\"/\",1)[-1] not in done]\n",
    "    if not pending:\n",
    "        print(\"Nada por descargar para esos años/meses. Caché al día.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Descargando {len(pending)} ZIP(s) a {RAW_ZIPS} …\")\n",
    "    results, errors = [], []\n",
    "    pos_pool = list(range(1, min(MAX_WORKERS, len(pending))+1))\n",
    "    pos_lock = threading.Lock()\n",
    "\n",
    "    def _task(k):\n",
    "        with pos_lock:\n",
    "            pos = pos_pool.pop() if pos_pool else 1\n",
    "        try:\n",
    "            name = download_one(k, position=pos)\n",
    "            return name\n",
    "        finally:\n",
    "            with pos_lock:\n",
    "                pos_pool.append(pos)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(pending))) as ex:\n",
    "        futs = {ex.submit(_task, k): k for k in pending}\n",
    "        for fut in as_completed(futs):\n",
    "            k = futs[fut]\n",
    "            try:\n",
    "                name = fut.result()\n",
    "                results.append(name)\n",
    "                d = load_dl_manifest(); d.add(name); save_dl_manifest(d)\n",
    "            except Exception as e:\n",
    "                errors.append((k, str(e)))\n",
    "\n",
    "    print(f\"Descargados: {len(results)} · Errores: {len(errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c036b844",
   "metadata": {},
   "source": [
    "\n",
    "# Procesamiento de Datos con Polars\n",
    "\n",
    "## ¿Qué hace esta celda?\n",
    "Procesa y limpia los datos de Citi Bike usando Polars:\n",
    "- Normaliza nombres de columnas inconsistentes entre años\n",
    "- Convierte tipos de datos apropiadamente (fechas, números, texto)\n",
    "- Filtra datos inválidos o anómalos (duraciones negativas, valores nulos)\n",
    "- Calcula métricas derivadas (duración en minutos)\n",
    "\n",
    "## Ventajas de Polars:\n",
    "- **Velocidad**: 10-100x más rápido que Pandas para datasets grandes\n",
    "- **Memoria**: Uso eficiente de RAM, maneja GB de datos sin problemas\n",
    "- **Lazy Evaluation**: Optimización automática de operaciones\n",
    "- **Type Safety**: Mejor manejo de tipos de datos\n",
    "\n",
    "## ¿Por qué es la mejor opción?\n",
    "- **Rendimiento**: Procesamiento ultra-rápido incluso con millones de filas\n",
    "- **Escalabilidad**: Maneja datasets grandes eficientemente\n",
    "- **Robustez**: Manejo robusto de datos inconsistentes entre años\n",
    "- **Mantenibilidad**: Código limpio y modular\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd22af73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T02:09:18.986701Z",
     "start_time": "2025-08-23T02:09:18.973903Z"
    }
   },
   "outputs": [],
   "source": [
    "COLMAP = {\n",
    "    \"start_time\": [\"start_time\",\"starttime\",\"started_at\"],\n",
    "    \"end_time\":   [\"end_time\",\"stoptime\",\"ended_at\"],\n",
    "    \"start_station_id\": [\"start_station_id\",\"start station id\",\"start_station_code\",\"from_station_id\"],\n",
    "    \"end_station_id\":   [\"end_station_id\",\"end station id\",\"end_station_code\",\"to_station_id\"],\n",
    "    \"start_station_name\": [\"start_station_name\",\"start station name\",\"from_station_name\"],\n",
    "    \"end_station_name\":   [\"end_station_name\",\"end station name\",\"to_station_name\"],\n",
    "    \"user_type\": [\"usertype\",\"user_type\",\"member_casual\",\"Subscriber type\"],\n",
    "    \"ride_id\": [\"ride_id\",\"trip_id\",\"id\",\"bikeid\",\"bike_id\"],\n",
    "    \"birth_year\": [\"birth_year\",\"birth year\",\"member_birth_year\"],\n",
    "    \"gender\": [\"gender\",\"member_gender\"],\n",
    "    \"tripduration\": [\"tripduration\",\"duration\",\"trip_duration\"],\n",
    "    \"start_lat\": [\"start station latitude\",\"start_lat\",\"start latitude\"],\n",
    "    \"start_lng\": [\"start station longitude\",\"start_lng\",\"start longitude\"],\n",
    "    \"end_lat\":   [\"end station latitude\",\"end_lat\",\"end latitude\"],\n",
    "    \"end_lng\":   [\"end station longitude\",\"end_lng\",\"end longitude\"],\n",
    "}\n",
    "REQUIRED = [\n",
    "    \"start_time\",\"end_time\",\n",
    "    \"start_station_id\",\"end_station_id\",\n",
    "    \"start_station_name\",\"end_station_name\",\n",
    "    \"ride_id\",\"user_type\"\n",
    "]\n",
    "NON_NULL_KEYS = [\"start_time\",\"end_time\",\"start_station_id\",\"end_station_id\"]\n",
    "\n",
    "# formatos de fecha más comunes en Citi Bike\n",
    "DT_FORMATS = [\n",
    "    \"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%d %H:%M\", \"%Y-%m-%d %H:%M:%S%.f\",\n",
    "    \"%m/%d/%Y %H:%M:%S\", \"%m/%d/%Y %H:%M\", \"%m/%d/%Y %H:%M:%S%.f\",\n",
    "    \"%Y-%m-%dT%H:%M:%S\", \"%Y-%m-%dT%H:%M:%S%.f\",\n",
    "]\n",
    "\n",
    "def canonicalize_columns(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    rename_map = {}\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    for canon, variants in COLMAP.items():\n",
    "        for v in variants:\n",
    "            lv = v.lower()\n",
    "            if lv in lower_map:\n",
    "                rename_map[lower_map[lv]] = canon\n",
    "                break\n",
    "    return df.rename(rename_map)\n",
    "\n",
    "def ensure_required(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    for col in REQUIRED:\n",
    "        if col not in df.columns:\n",
    "            df = df.with_columns(pl.lit(None).alias(col))\n",
    "    return df\n",
    "\n",
    "def parse_datetime_multi(df: pl.DataFrame, col: str) -> pl.DataFrame:\n",
    "    if col not in df.columns:\n",
    "        return df\n",
    "    s = (\n",
    "        pl.col(col).cast(pl.Utf8, strict=False)\n",
    "        .str.replace_all(r\"[Tt]\", \" \", literal=False)\n",
    "        .str.replace_all(r\"[Zz]$\", \"\", literal=False)\n",
    "        .str.replace_all(r\"[+-]\\d{2}:?\\d{2}$\", \"\", literal=False)\n",
    "    )\n",
    "    df = df.with_columns(s.alias(col))\n",
    "    best = None; best_ok = -1\n",
    "    for fmt in DT_FORMATS:\n",
    "        try:\n",
    "            cand = pl.col(col).str.strptime(pl.Datetime, format=fmt, strict=False)\n",
    "            ok = df.select(cand.is_not_null().sum()).item()\n",
    "            if ok > best_ok:\n",
    "                best, best_ok = cand, ok\n",
    "            if ok >= 0.8*len(df):\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "    if best is not None:\n",
    "        df = df.with_columns(best.alias(col))\n",
    "    # último intento de inferencia laxa\n",
    "    nulls = df.select(pl.col(col).is_null().sum()).item()\n",
    "    if nulls > 0.2*len(df):\n",
    "        df = df.with_columns(pl.col(col).str.to_datetime(strict=False).alias(col))\n",
    "    return df\n",
    "\n",
    "def cast_and_clean(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    if \"start_time\" in df.columns: df = parse_datetime_multi(df, \"start_time\")\n",
    "    if \"end_time\" in df.columns:   df = parse_datetime_multi(df, \"end_time\")\n",
    "\n",
    "    for c in [\"start_station_id\",\"end_station_id\",\"ride_id\",\"user_type\",\n",
    "              \"start_station_name\",\"end_station_name\",\"gender\",\"start_lat\",\"start_lng\",\"end_lat\",\"end_lng\"]:\n",
    "        if c in df.columns and df.schema.get(c) != pl.Utf8:\n",
    "            df = df.with_columns(pl.col(c).cast(pl.Utf8, strict=False))\n",
    "\n",
    "    if \"birth_year\" in df.columns and df.schema.get(\"birth_year\") != pl.Int64:\n",
    "        df = df.with_columns(pl.col(\"birth_year\").cast(pl.Int64, strict=False))\n",
    "\n",
    "    if \"tripduration\" in df.columns:\n",
    "        df = df.with_columns((pl.col(\"tripduration\").cast(pl.Float64, strict=False)/60).alias(\"trip_duration_min\"))\n",
    "    elif \"start_time\" in df.columns and \"end_time\" in df.columns:\n",
    "        df = df.with_columns(((pl.col(\"end_time\")-pl.col(\"start_time\")).dt.total_seconds()/60).alias(\"trip_duration_min\"))\n",
    "\n",
    "    if \"trip_duration_min\" in df.columns:\n",
    "        df = df.filter((pl.col(\"trip_duration_min\")>=0) & (pl.col(\"trip_duration_min\")<=1440))\n",
    "\n",
    "    for c in NON_NULL_KEYS:\n",
    "        if c in df.columns:\n",
    "            df = df.filter(pl.col(c).is_not_null())\n",
    "\n",
    "    if \"ride_id\" in df.columns:\n",
    "        df = df.unique(subset=[\"ride_id\"], keep=\"first\")\n",
    "    else:\n",
    "        subset_cols = [c for c in [\"start_time\",\"end_time\",\"start_station_id\",\"end_station_id\"] if c in df.columns]\n",
    "        if subset_cols:\n",
    "            df = df.unique(subset=subset_cols, keep=\"first\")\n",
    "\n",
    "    df = ensure_required(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc54512",
   "metadata": {},
   "source": [
    "\n",
    "# Conversión a Parquet Particionado\n",
    "\n",
    "## ¿Qué hace esta celda?\n",
    "Convierte los datos CSV a formato Parquet particionado:\n",
    "- Extrae datos de archivos ZIP anidados (algunos ZIPs contienen otros ZIPs)\n",
    "- Particiona automáticamente por año/mes para consultas ultra-rápidas\n",
    "- Procesa múltiples archivos en paralelo para máxima eficiencia\n",
    "- Optimiza el almacenamiento y consultas futuras\n",
    "\n",
    "## Ventajas del Formato Parquet:\n",
    "- **Compresión**: 80-90% de ahorro de espacio vs CSV\n",
    "- **Particionado**: Consultas ultra-rápidas por fecha (solo lee particiones relevantes)\n",
    "- **Columnar**: Ideal para análisis de datos y agregaciones\n",
    "- **Compatibilidad**: Funciona con todas las herramientas (Pandas, Spark, etc.)\n",
    "\n",
    "## ¿Por qué es la mejor opción?\n",
    "- **Eficiencia**: Consultas 10-100x más rápidas que CSV\n",
    "- **Escalabilidad**: Maneja terabytes de datos sin problemas\n",
    "- **Costos**: Reduce significativamente costos de almacenamiento\n",
    "- **Flexibilidad**: Permite consultas selectivas por partición\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6036267f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T02:09:20.059432Z",
     "start_time": "2025-08-23T02:09:20.027636Z"
    }
   },
   "outputs": [],
   "source": [
    "def infer_year_month_from_name(name: str):\n",
    "    m = re.search(r\"(20\\d{2})(\\d{2})-citibike-tripdata\", name)\n",
    "    if m: return int(m.group(1)), int(m.group(2))\n",
    "    m2 = re.search(r\"(20\\d{2})\", name)\n",
    "    if m2: return int(m2.group(1)), None\n",
    "    return None, None\n",
    "\n",
    "def infer_year_month_from_data(df: pl.DataFrame):\n",
    "    if \"start_time\" in df.columns:\n",
    "        s = df.select(pl.col(\"start_time\").drop_nulls()).to_series()\n",
    "        if len(s)>0 and s[0] is not None: return s[0].year, s[0].month\n",
    "    return None, None\n",
    "\n",
    "def partition_done(year:int, month:int|None):\n",
    "    m = month if month is not None else 0\n",
    "    part_dir = OUTPUT_DIR / f\"year={year}\" / f\"month={m:02d}\"\n",
    "    return (part_dir / \"_SUCCESS\").exists()\n",
    "\n",
    "def write_parquet_partition(df: pl.DataFrame, year: int, month: int | None):\n",
    "    m = month if month is not None else 0\n",
    "    part_dir = OUTPUT_DIR / f\"year={year}\" / f\"month={m:02d}\"\n",
    "    part_dir.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = part_dir / f\"_tmp_{uuid.uuid4().hex}.parquet\"\n",
    "    final = part_dir / f\"part-{uuid.uuid4().hex}.parquet\"\n",
    "    df.write_parquet(tmp, compression=\"snappy\")\n",
    "    tmp.replace(final)\n",
    "    (part_dir / \"_SUCCESS\").touch(exist_ok=True)\n",
    "\n",
    "def process_csv_bytes(csv_bytes: bytes, csv_name: str) -> int:\n",
    "    df = pl.read_csv(io.BytesIO(csv_bytes), ignore_errors=True)\n",
    "    df = canonicalize_columns(df)\n",
    "    df = cast_and_clean(df)\n",
    "\n",
    "    y, m = infer_year_month_from_name(csv_name)\n",
    "    if y is None: y, m = infer_year_month_from_data(df)\n",
    "    if y is None: return 0\n",
    "\n",
    "    if partition_done(y, m):\n",
    "        return 0\n",
    "\n",
    "    write_parquet_partition(df, y, m)\n",
    "    n = df.height\n",
    "    del df; gc.collect()\n",
    "    return n\n",
    "\n",
    "def months_filter_ok(csv_name: str, allowed_months: set[int]|None) -> bool:\n",
    "    if not allowed_months:\n",
    "        return True\n",
    "    m = re.search(r\"(20\\d{2})(\\d{2})-citibike-tripdata\", csv_name)\n",
    "    if m:\n",
    "        mm = int(m.group(2))\n",
    "        return mm in allowed_months\n",
    "    return True\n",
    "\n",
    "def process_zip_file(zip_path: Path, allowed_months: set[int]|None=None, position:int=1) -> int:\n",
    "    total_rows = 0\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as outer:\n",
    "        csv_members = []\n",
    "        for member in outer.namelist():\n",
    "            if \"__MACOSX\" in member or member.endswith(\".DS_Store\") or member.startswith(\"._\"):\n",
    "                continue\n",
    "            if member.endswith(\".zip\"):\n",
    "                with outer.open(member) as inner_bytes:\n",
    "                    with zipfile.ZipFile(io.BytesIO(inner_bytes.read()), \"r\") as inner_zip:\n",
    "                        for inner_member in inner_zip.namelist():\n",
    "                            if inner_member.endswith(\".csv\") and months_filter_ok(inner_member, allowed_months):\n",
    "                                csv_members.append((\"inner\", member, inner_member))\n",
    "            elif member.endswith(\".csv\") and months_filter_ok(member, allowed_months):\n",
    "                csv_members.append((\"outer\", None, member))\n",
    "\n",
    "        pbar = tqdm(total=len(csv_members), desc=f\"Procesando {zip_path.name}\", position=position, unit=\"csv\", leave=True, dynamic_ncols=True)\n",
    "\n",
    "        for kind, parent, csvm in csv_members:\n",
    "            try:\n",
    "                if kind == \"inner\":\n",
    "                    with outer.open(parent) as inner_bytes:\n",
    "                        with zipfile.ZipFile(io.BytesIO(inner_bytes.read()), \"r\") as inner_zip:\n",
    "                            with inner_zip.open(csvm) as f:\n",
    "                                total_rows += process_csv_bytes(f.read(), csvm)\n",
    "                else:\n",
    "                    with outer.open(csvm) as f:\n",
    "                        total_rows += process_csv_bytes(f.read(), csvm)\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"Error: {zip_path.name} / {csvm}: {e}\")\n",
    "            finally:\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "    return total_rows\n",
    "\n",
    "def process_cache_to_parquet(years: list[int]|None=None, months: set[int]|None=None):\n",
    "    \"\"\"\n",
    "    Recorre los ZIPs en raw_zips/ y procesa a Parquet.\n",
    "    Si 'years' se especifica, filtra ZIPs por año en su nombre.\n",
    "    Si 'months' se especifica, solo escribe esos meses (particiones).\n",
    "    \"\"\"\n",
    "    zips = sorted(RAW_ZIPS.glob(\"*.zip\"))\n",
    "    if years:\n",
    "        zips = [z for z in zips if any(str(y) in z.name for y in years)]\n",
    "    if not zips:\n",
    "        print(\"No hay ZIPs en raw_zips/. Descarga primero en la Celda 2.\")\n",
    "        return\n",
    "\n",
    "    # Filtrar solo los ZIPs que no han sido procesados\n",
    "    processed = load_proc_manifest()\n",
    "    zips = [z for z in zips if z.name not in processed]\n",
    "\n",
    "    print(f\"Procesando {len(zips)} ZIP(s) desde caché hacia parquet…\")\n",
    "    pos_pool = list(range(1, min(MAX_WORKERS, len(zips)) + 1))\n",
    "    pos_lock = threading.Lock()\n",
    "\n",
    "    results, errors = [], []\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    with ThreadPoolExecutor(max_workers=min(MAX_WORKERS, len(zips))) as ex:\n",
    "        def _task(z: Path):\n",
    "            with pos_lock:\n",
    "                pos = pos_pool.pop() if pos_pool else 1\n",
    "            try:\n",
    "                rows = process_zip_file(z, allowed_months=months, position=pos)\n",
    "                return z.name, rows\n",
    "            finally:\n",
    "                with pos_lock:\n",
    "                    pos_pool.append(pos)\n",
    "\n",
    "        futs = {ex.submit(_task, z): z for z in zips}\n",
    "        for fut in as_completed(futs):\n",
    "            z = futs[fut]\n",
    "            try:\n",
    "                name, rows = fut.result()\n",
    "                results.append((name, rows))\n",
    "                d = load_proc_manifest(); d.add(name); save_proc_manifest(d)\n",
    "            except Exception as e:\n",
    "                errors.append((z.name, str(e)))\n",
    "\n",
    "    print(\"\\nResumen:\")\n",
    "    for n, r in results:\n",
    "        print(f\" - {n}: {r} filas\")\n",
    "    if errors:\n",
    "        print(\"\\nErrores:\")\n",
    "        for n, e in errors:\n",
    "            print(f\" - {n}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f39d660",
   "metadata": {},
   "source": [
    "\n",
    "# Funciones de Utilidad y Mantenimiento\n",
    "\n",
    "## ¿Qué hace esta celda?\n",
    "Define funciones auxiliares para el mantenimiento y gestión del pipeline:\n",
    "- **list_cache_for_year()**: Muestra qué archivos están en caché para un año específico\n",
    "- **drop_year()**: Elimina completamente los datos de un año (para reprocesamiento)\n",
    "- **drop_months()**: Elimina meses específicos de un año\n",
    "- **reprocess_year()**: Reprocesa un año completo desde cero\n",
    "- **reprocess_months()**: Reprocesa meses específicos de un año\n",
    "\n",
    "## Funcionalidades Clave:\n",
    "- **Inspección**: Ver qué datos están disponibles en caché\n",
    "- **Limpieza Selectiva**: Eliminar datos específicos sin afectar el resto\n",
    "- **Reprocesamiento**: Reprocesar datos cuando hay errores o cambios\n",
    "- **Debugging**: Herramientas para identificar y corregir problemas\n",
    "\n",
    "## ¿Por qué es la mejor opción?\n",
    "- **Control Granular**: Puedes trabajar con años o meses específicos\n",
    "- **Debugging Eficiente**: Fácil identificación y corrección de problemas\n",
    "- **Mantenimiento**: Operaciones comunes predefinidas y seguras\n",
    "- **Flexibilidad**: Adapta el procesamiento a tus necesidades específicas\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0fc07ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T02:09:20.564220Z",
     "start_time": "2025-08-23T02:09:20.559806Z"
    }
   },
   "outputs": [],
   "source": [
    "def list_cache_for_year(year: int):\n",
    "    zips = sorted(RAW_ZIPS.glob(\"*.zip\"))\n",
    "    monthly = [z.name for z in zips if re.fullmatch(fr\"{year}\\d{{2}}-citibike-tripdata\\.zip\", z.name)]\n",
    "    annual  = [z.name for z in zips if z.name == f\"{year}-citibike-tripdata.zip\"]\n",
    "    print(f\"Año {year} en caché:\")\n",
    "    if annual:\n",
    "        print(\" - ZIP anual:\", \", \".join(annual))\n",
    "    if monthly:\n",
    "        print(\" - ZIPs mensuales:\", \", \".join(sorted(monthly)))\n",
    "    if not annual and not monthly:\n",
    "        print(\" - No hay ZIPs de este año en raw_zips/\")\n",
    "\n",
    "def drop_year(year: int):\n",
    "    ydir = OUTPUT_DIR / f\"year={year}\"\n",
    "    if ydir.exists():\n",
    "        shutil.rmtree(ydir, ignore_errors=True)\n",
    "        print(f\"Eliminado: {ydir}\")\n",
    "    else:\n",
    "        print(f\"No existe: {ydir}\")\n",
    "\n",
    "def drop_months(year: int, months: list[int]):\n",
    "    for m in months:\n",
    "        p = OUTPUT_DIR / f\"year={year}\" / f\"month={m:02d}\"\n",
    "        if p.exists():\n",
    "            shutil.rmtree(p, ignore_errors=True)\n",
    "            print(f\"Eliminado: {p}\")\n",
    "        else:\n",
    "            print(f\"No existe: {p}\")\n",
    "\n",
    "def reprocess_year(year: int):\n",
    "    list_cache_for_year(year)\n",
    "    process_cache_to_parquet(years=[year], months=None)\n",
    "\n",
    "def reprocess_months(year: int, months: list[int]):\n",
    "    list_cache_for_year(year)\n",
    "    process_cache_to_parquet(years=[year], months=set(months))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c271830",
   "metadata": {},
   "source": [
    "\n",
    "# Comandos de Ejecución y Utilidades\n",
    "\n",
    "## ¿Qué hace esta celda?\n",
    "Proporciona comandos de alto nivel para ejecutar el pipeline completo:\n",
    "- **Descarga completa**: Descarga todos los datos disponibles\n",
    "- **Procesamiento completo**: Convierte todos los datos a Parquet\n",
    "- **Comandos comentados**: Ejemplos de operaciones específicas por año/mes\n",
    "\n",
    "## Comandos Disponibles:\n",
    "- **download_to_cache()**: Descarga todos los archivos faltantes\n",
    "- **process_cache_to_parquet()**: Procesa todos los archivos a Parquet\n",
    "- **download_years_months_to_cache()**: Descarga años/meses específicos\n",
    "- **drop_year() / reprocess_year()**: Limpia y reprocesa años completos\n",
    "- **drop_months() / reprocess_months()**: Limpia y reprocesa meses específicos\n",
    "\n",
    "## ¿Por qué es la mejor opción?\n",
    "- **Simplicidad**: Comandos de una línea para operaciones complejas\n",
    "- **Flexibilidad**: Control total sobre qué procesar y cuándo\n",
    "- **Eficiencia**: Solo procesa lo que necesitas\n",
    "- **Debugging**: Fácil identificación y corrección de problemas\n",
    "- **Mantenimiento**: Operaciones comunes predefinidas y documentadas\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0bfc926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T02:12:31.889431Z",
     "start_time": "2025-08-23T02:09:21.154571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando 3 ZIP(s) a raw_zips con 4 hilos…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c293d8378cd4b038731bebba4a26114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Descargando  2015-citibike-tripdata.zip:   0%|           0.00/264M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f593d7b3364b2abc88d879a56e65ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Descargando  2013-citibike-tripdata.zip:   0%|           0.00/315M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11e1834abe24ca0adf1e1a74bbb4d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Descargando  2014-citibike-tripdata.zip:   0%|           0.00/214M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargados: 3 · Errores: 0\n",
      "Procesando 3 ZIP(s) desde caché hacia parquet…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4bc027afde435597795f8885596ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando 2015-citibike-tripdata.zip:   0%|          | 0/16 [00:00<?, ?csv/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bdb3a2c0c94956b2725188ab34fcfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando 2014-citibike-tripdata.zip:   0%|          | 0/12 [00:00<?, ?csv/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e59c157c434ef5b777e42e9edba10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando 2013-citibike-tripdata.zip:   0%|          | 0/17 [00:00<?, ?csv/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen:\n",
      " - 2013-citibike-tripdata.zip: 40458 filas\n",
      " - 2014-citibike-tripdata.zip: 71754 filas\n",
      " - 2015-citibike-tripdata.zip: 74942 filas\n"
     ]
    }
   ],
   "source": [
    "# Descargar todo (o pon un número para prueba rápida)\n",
    "download_to_cache(max_objects=None)\n",
    "# download_to_cache(max_objects=5)\n",
    "\n",
    "# Descargar enfocado por año/mes\n",
    "# download_years_months_to_cache([2017], months={3,4,5})\n",
    "\n",
    "# Procesar todo lo que hay en caché → Parquet\n",
    "process_cache_to_parquet(years=None, months=None)\n",
    "\n",
    "# Borrar y reprocesar un año completo\n",
    "# drop_year(2017)\n",
    "# reprocess_year(2017)\n",
    "\n",
    "# Borrar y reprocesar meses puntuales\n",
    "# drop_months(2017, [3,4,5])\n",
    "# reprocess_months(2017, [3,4,5]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
